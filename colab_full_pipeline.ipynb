{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9619797",
   "metadata": {},
   "source": [
    "## 1. Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ded334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q sentence-transformers chromadb scikit-learn transformers accelerate bitsandbytes pymongo pdfplumber torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac3def1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc89c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Navigate to project directory\n",
    "%cd /content/drive/MyDrive/bachelor_project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b8d5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add src directory to Python path\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Get current working directory\n",
    "current_dir = os.getcwd()\n",
    "src_path = os.path.join(current_dir, 'src')\n",
    "\n",
    "# Add to path if not already there\n",
    "if src_path not in sys.path:\n",
    "    sys.path.insert(0, src_path)\n",
    "    print(f\"‚úì Added to path: {src_path}\")\n",
    "\n",
    "# Verify src files exist\n",
    "if os.path.exists(src_path):\n",
    "    src_files = os.listdir(src_path)\n",
    "    print(f\"‚úì Found {len(src_files)} files in src/\")\n",
    "    print(f\"  Files: {', '.join([f for f in src_files if f.endswith('.py')])}\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è Warning: src/ directory not found at {src_path}\")\n",
    "    print(\"Make sure you've uploaded your project files to Google Drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f331d2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check current directory structure\n",
    "import os\n",
    "\n",
    "print(\"üìÅ Current directory:\", os.getcwd())\n",
    "print(\"\\nüìÇ Directory contents:\")\n",
    "for item in os.listdir('.'):\n",
    "    item_type = \"üìÅ\" if os.path.isdir(item) else \"üìÑ\"\n",
    "    print(f\"  {item_type} {item}\")\n",
    "\n",
    "# Check if src exists\n",
    "if os.path.exists('src'):\n",
    "    print(\"\\n‚úì src/ folder found!\")\n",
    "    print(\"üìÇ Files in src/:\")\n",
    "    for item in os.listdir('src'):\n",
    "        if item.endswith('.py'):\n",
    "            print(f\"  üìÑ {item}\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è src/ folder not found!\")\n",
    "    \n",
    "# Check for txt files\n",
    "txt_files = [f for f in os.listdir('.') if f.endswith('.txt')]\n",
    "if txt_files:\n",
    "    print(f\"\\n‚úì Found {len(txt_files)} .txt file(s):\")\n",
    "    for f in txt_files:\n",
    "        print(f\"  üìÑ {f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264048fd",
   "metadata": {},
   "source": [
    "## 2. Load Llama 3.1 Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ed6ca3",
   "metadata": {},
   "source": [
    "**Model Se√ßenekleri:**\n",
    "- **Llama 3.2 3B**: Meta'nƒ±n son modeli, hafif (HF token gerekli) (√ñNERƒ∞LEN)\n",
    "- **Llama 3.1 8B**: Daha g√º√ßl√º ama aƒüƒ±r (HF token gerekli)\n",
    "- **Qwen 2.5 7B**: T√ºrk√ße dahil √ßokdilli, token gerekmez\n",
    "- **Mistral 7B**: A√ßƒ±k eri≈üimli, token gerekmez\n",
    "\n",
    "Llama kullanmak i√ßin HuggingFace token'ƒ±nƒ± Colab Secrets'a eklemelisin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e51f13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77500279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get HuggingFace token from Colab Secrets\n",
    "from google.colab import userdata\n",
    "import os\n",
    "\n",
    "try:\n",
    "    HF_TOKEN = userdata.get('HF_TOKEN')\n",
    "    os.environ['HF_TOKEN'] = HF_TOKEN\n",
    "    print(\"‚úì HuggingFace token loaded from Colab Secrets\")\n",
    "    print(f\"Token length: {len(HF_TOKEN)} characters\")\n",
    "    print(f\"Token preview: {HF_TOKEN[:10]}...{HF_TOKEN[-10:]}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Token bulunamadƒ±: {e}\")\n",
    "    print(\"\\n=== TOKEN EKLEME ADIMLARI ===\")\n",
    "    print(\"1. Sol panelde üîë (Key/Secrets) simgesine tƒ±kla\")\n",
    "    print(\"2. 'Add new secret' butonuna bas\")\n",
    "    print(\"3. Name: HF_TOKEN\")\n",
    "    print(\"4. Value: HuggingFace token'ƒ±nƒ± yapƒ±≈ütƒ±r\")\n",
    "    print(\"5. 'Notebook access' toggle'ƒ±nƒ± A√á (√∂nemli!)\")\n",
    "    print(\"6. Bu h√ºcreyi tekrar √ßalƒ±≈ütƒ±r\")\n",
    "    print(\"\\nToken almak i√ßin: https://huggingface.co/settings/tokens\")\n",
    "    HF_TOKEN = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a227133",
   "metadata": {},
   "source": [
    "### HuggingFace Token Setup\n",
    "\n",
    "**Colab Secrets'a token eklemek i√ßin:**\n",
    "1. Sol panelde üîë (Key) simgesine tƒ±kla\n",
    "2. \"Add new secret\" butonuna bas\n",
    "3. Name: `HF_TOKEN`\n",
    "4. Value: HuggingFace token'ƒ±nƒ± yapƒ±≈ütƒ±r (https://huggingface.co/settings/tokens)\n",
    "5. \"Notebook access\" toggle'ƒ±nƒ± a√ß\n",
    "\n",
    "Token eklendikten sonra a≈üaƒüƒ±daki h√ºcreleri √ßalƒ±≈ütƒ±r."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbd9fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Model for Summarization\n",
    "# Run this cell when you're ready to generate summaries (Step 6)\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import os\n",
    "\n",
    "# Option 3: Qwen 2.5 7B (no token needed, excellent Turkish support) - RECOMMENDED\n",
    "MODEL_ID = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "\n",
    "# Option 4: Mistral 7B (no token needed)\n",
    "# MODEL_ID = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "\n",
    "# Option 1: Llama 3.2 3B (gated, requires HF token)\n",
    "# MODEL_ID = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "\n",
    "# Option 2: Llama 3.1 8B (gated, requires HF token, more powerful)\n",
    "# MODEL_ID = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "\n",
    "# 4-bit quantization config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "print(f\"Loading tokenizer from {MODEL_ID}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "\n",
    "print(\"Loading model (this may take a few minutes)...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "print(\"‚úì Model loaded successfully!\")\n",
    "print(f\"Model: {MODEL_ID}\")\n",
    "print(f\"Device: {model.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1630933",
   "metadata": {},
   "source": [
    "## 3. Parse and Prepare Screenplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006e9f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "# Specify your script file (can be .txt or .pdf)\n",
    "script_file = \"the_addams_family.txt\"  # Change this to your file\n",
    "\n",
    "# Metadata: Update based on your content type\n",
    "metadata = {\n",
    "    \"title\": \"The Addams Family\",  # Film or series name\n",
    "    \"type\": \"movie\",  # \"movie\" or \"series\"\n",
    "    \"year\": 1991,  # Optional\n",
    "    # For series, add: \"season\": 1, \"episode\": 1\n",
    "}\n",
    "\n",
    "# Check file extension and process accordingly\n",
    "if script_file.endswith('.txt'):\n",
    "    txt_file = script_file\n",
    "    print(f\"‚úì Using existing text file: {txt_file}\")\n",
    "elif script_file.endswith('.pdf'):\n",
    "    from parser_pdf_to_txt import PDFParser\n",
    "    parser = PDFParser(output_dir=\"data/raw_scripts\")\n",
    "    txt_file = parser.parse_pdf(script_file)\n",
    "    print(f\"‚úì Parsed PDF to: {txt_file}\")\n",
    "else:\n",
    "    raise ValueError(f\"Unsupported file format: {script_file}. Use .txt or .pdf\")\n",
    "\n",
    "# Verify file exists\n",
    "if not os.path.exists(txt_file):\n",
    "    raise FileNotFoundError(f\"File not found: {txt_file}\")\n",
    "\n",
    "# ========== PREPROCESSING: Clean screenplay text (INLINE) ==========\n",
    "print(\"\\nüìã PREPROCESSING: Cleaning screenplay...\")\n",
    "\n",
    "# Define cleaner inline\n",
    "patterns = {\n",
    "    'scene_header': r'^(INT\\.|EXT\\.)\\s+[^\\n]*(DAY|NIGHT|DAWN|DUSK|CONTINUOUS)',\n",
    "    'transition': r'^(CUT TO|FADE IN|FADE OUT|DISSOLVE TO|SMASH CUT)\\b',\n",
    "    'technical': r'\\b(CAMERA|SOUND|MUSIC|MONTAGE|TITLE CARD)\\b',\n",
    "    'parenthetical': r'\\([^)]*(?:whispering|shouting|V\\.O\\.|O\\.S\\.|beat|pause)\\)',\n",
    "    'whitespace': r'\\s+',\n",
    "    'formatting': r'^[-=*]{3,}$',\n",
    "}\n",
    "\n",
    "def clean_screenplay(text, min_line_length=5):\n",
    "    lines = text.split('\\n')\n",
    "    cleaned_lines = []\n",
    "    \n",
    "    for line in lines:\n",
    "        original = line.strip()\n",
    "        if not original:\n",
    "            continue\n",
    "        \n",
    "        # Skip scene headers, transitions, formatting\n",
    "        if (re.match(patterns['scene_header'], original, re.IGNORECASE) or\n",
    "            re.match(patterns['formatting'], original) or\n",
    "            re.match(patterns['transition'], original, re.IGNORECASE)):\n",
    "            continue\n",
    "        \n",
    "        # Clean the line\n",
    "        cleaned = re.sub(patterns['parenthetical'], '', original, flags=re.IGNORECASE)\n",
    "        cleaned = re.sub(patterns['technical'], '', cleaned, flags=re.IGNORECASE)\n",
    "        cleaned = re.sub(patterns['whitespace'], ' ', cleaned).strip()\n",
    "        \n",
    "        if cleaned and len(cleaned) >= min_line_length:\n",
    "            cleaned_lines.append(cleaned)\n",
    "    \n",
    "    result = '\\n'.join(cleaned_lines)\n",
    "    result = re.sub(r'\\n{3,}', '\\n\\n', result)\n",
    "    return result.strip()\n",
    "\n",
    "# Read and clean\n",
    "with open(txt_file, 'r', encoding='utf-8') as f:\n",
    "    original_text = f.read()\n",
    "\n",
    "cleaned_text = clean_screenplay(original_text)\n",
    "\n",
    "# Statistics\n",
    "orig_lines = len(original_text.split('\\n'))\n",
    "clean_lines = len(cleaned_text.split('\\n'))\n",
    "orig_chars = len(original_text)\n",
    "clean_chars = len(cleaned_text)\n",
    "noise_pct = round((1 - clean_chars / orig_chars) * 100, 1) if orig_chars > 0 else 0\n",
    "\n",
    "print(\"\\nüìä Cleaning Statistics:\")\n",
    "print(f\"  Lines: {orig_lines} ‚Üí {clean_lines} ({orig_lines - clean_lines} removed, {round((orig_lines-clean_lines)/orig_lines*100, 1)}%)\")\n",
    "print(f\"  Characters: {orig_chars:,} ‚Üí {clean_chars:,}\")\n",
    "print(f\"  Noise removed: {noise_pct}%\")\n",
    "\n",
    "# Save cleaned version\n",
    "txt_file_cleaned = txt_file.replace('.txt', '_cleaned.txt')\n",
    "with open(txt_file_cleaned, 'w', encoding='utf-8') as f:\n",
    "    f.write(cleaned_text)\n",
    "\n",
    "txt_file = txt_file_cleaned\n",
    "print(f\"\\n‚úì Using cleaned text: {txt_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86adee15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into scenes\n",
    "from scene_splitter import SceneSplitter\n",
    "\n",
    "# Create output directory based on content type\n",
    "if metadata[\"type\"] == \"movie\":\n",
    "    output_dir = f\"data/scenes/{metadata['title'].replace(' ', '_').lower()}\"\n",
    "else:\n",
    "    output_dir = f\"data/scenes/{metadata['title'].replace(' ', '_').lower()}_s{metadata.get('season', 1):02d}e{metadata.get('episode', 1):02d}\"\n",
    "\n",
    "splitter = SceneSplitter(output_dir=output_dir)\n",
    "scenes = splitter.process_script(txt_file, metadata=metadata)\n",
    "\n",
    "print(f\"‚úì Split into {len(scenes)} scenes\")\n",
    "print(f\"‚úì Saved to: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa417376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk long scenes\n",
    "from chunker import SceneChunker\n",
    "\n",
    "# Create chunks directory and prefix based on content\n",
    "if metadata[\"type\"] == \"movie\":\n",
    "    chunks_dir = f\"data/chunks/{metadata['title'].replace(' ', '_').lower()}\"\n",
    "    prefix = metadata['title'].replace(' ', '_').lower()\n",
    "else:\n",
    "    chunks_dir = f\"data/chunks/{metadata['title'].replace(' ', '_').lower()}_s{metadata.get('season', 1):02d}e{metadata.get('episode', 1):02d}\"\n",
    "    prefix = f\"{metadata['title'].replace(' ', '_').lower()}_s{metadata.get('season', 1):02d}e{metadata.get('episode', 1):02d}\"\n",
    "\n",
    "chunker = SceneChunker(output_dir=chunks_dir)\n",
    "chunks = chunker.process_all_scenes(output_dir, prefix=prefix)\n",
    "\n",
    "print(f\"‚úì Created {len(chunks)} chunks\")\n",
    "print(f\"‚úì Saved to: {chunks_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3597b0b",
   "metadata": {},
   "source": [
    "## 4. Generate Embeddings and Store in ChromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab2436c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from embedder import EmbeddingManager\n",
    "from vectorstore import VectorStore\n",
    "\n",
    "# Generate embeddings\n",
    "embedder = EmbeddingManager()\n",
    "chunks_with_embeddings = embedder.add_embeddings_to_chunks(chunks)\n",
    "\n",
    "print(f\"‚úì Generated embeddings for {len(chunks_with_embeddings)} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28740489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store in ChromaDB\n",
    "vectorstore = VectorStore(persist_directory=\"embeddings\")\n",
    "collection_name = \"your_show_s01e01\"\n",
    "\n",
    "vectorstore.add_chunks_to_collection(collection_name, chunks_with_embeddings)\n",
    "\n",
    "print(f\"‚úì Stored in collection: {collection_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ce9ae9",
   "metadata": {},
   "source": [
    "## 5. Select Representative Scenes (BRV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69631ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from clustering import RepresentativeSceneSelector\n",
    "import numpy as np\n",
    "\n",
    "# Extract embeddings\n",
    "embeddings = np.array([chunk['embedding'] for chunk in chunks_with_embeddings])\n",
    "\n",
    "# Select representatives\n",
    "selector = RepresentativeSceneSelector(n_clusters=10)\n",
    "representatives = selector.select_brv_scenes(chunks_with_embeddings, auto_select_k=True)\n",
    "\n",
    "print(f\"‚úì Selected {len(representatives)} representative scenes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34125ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview representatives\n",
    "for i, rep in enumerate(representatives[:5], 1):\n",
    "    print(f\"\\n{i}. {rep['header']}\")\n",
    "    print(f\"   Scene: {rep['scene_number']}, Cluster: {rep['cluster_id']}\")\n",
    "    print(f\"   Preview: {rep['content'][:150]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339b8856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize embeddings in 2D space\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import numpy as np\n",
    "\n",
    "# Extract data for visualization\n",
    "embeddings_array = np.array([chunk['embedding'] for chunk in chunks_with_embeddings])\n",
    "cluster_labels = np.array([chunk.get('cluster_id', -1) for chunk in chunks_with_embeddings])\n",
    "scene_numbers = [chunk['scene_number'] for chunk in chunks_with_embeddings]\n",
    "\n",
    "# Reduce to 2D using t-SNE\n",
    "print(\"üîÑ Reducing embeddings to 2D (this may take a minute)...\")\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=min(30, len(embeddings_array)-1))\n",
    "embeddings_2d = tsne.fit_transform(embeddings_array)\n",
    "\n",
    "# Create figure\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "# Plot all chunks\n",
    "unique_clusters = np.unique(cluster_labels)\n",
    "colors = plt.cm.tab10(np.linspace(0, 1, len(unique_clusters)))\n",
    "\n",
    "for i, cluster_id in enumerate(unique_clusters):\n",
    "    mask = cluster_labels == cluster_id\n",
    "    plt.scatter(\n",
    "        embeddings_2d[mask, 0], \n",
    "        embeddings_2d[mask, 1],\n",
    "        c=[colors[i]],\n",
    "        label=f'Cluster {int(cluster_id)}',\n",
    "        alpha=0.6,\n",
    "        s=100\n",
    "    )\n",
    "\n",
    "# Highlight representative chunks\n",
    "rep_indices = [chunk['chunk_id']-1 for chunk in representatives]  # chunk_id is 1-indexed\n",
    "rep_coords = embeddings_2d[rep_indices]\n",
    "\n",
    "plt.scatter(\n",
    "    rep_coords[:, 0], \n",
    "    rep_coords[:, 1],\n",
    "    c='red',\n",
    "    marker='*',\n",
    "    s=500,\n",
    "    edgecolors='black',\n",
    "    linewidths=2,\n",
    "    label='Representatives',\n",
    "    zorder=5\n",
    ")\n",
    "\n",
    "# Add labels for representatives\n",
    "for i, (x, y) in enumerate(rep_coords):\n",
    "    plt.annotate(\n",
    "        f\"R{i+1}\",\n",
    "        (x, y),\n",
    "        xytext=(5, 5),\n",
    "        textcoords='offset points',\n",
    "        fontsize=10,\n",
    "        fontweight='bold',\n",
    "        bbox=dict(boxstyle='round,pad=0.3', facecolor='yellow', alpha=0.7)\n",
    "    )\n",
    "\n",
    "plt.title('Chunk Embeddings Visualization (2D t-SNE)', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('t-SNE Dimension 1', fontsize=12)\n",
    "plt.ylabel('t-SNE Dimension 2', fontsize=12)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"‚úì Visualized {len(embeddings_array)} chunks in {len(unique_clusters)} clusters\")\n",
    "print(f\"‚≠ê {len(representatives)} representative chunks highlighted in red\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9446100",
   "metadata": {},
   "source": [
    "### Visualize Clusters\n",
    "\n",
    "Embedding'leri 2D'ye d√º≈ü√ºr√ºp cluster'larƒ± g√∂rselle≈ütirelim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce2f490",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "13a09b59",
   "metadata": {},
   "source": [
    "## 6. Generate Summary (Map-Reduce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ae0d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAP phase: Summarize each representative scene\n",
    "# ‚ö†Ô∏è Make sure you've loaded the model first (Step 2, cell 11)\n",
    "\n",
    "# Check if model and tokenizer are loaded\n",
    "try:\n",
    "    _ = tokenizer\n",
    "    _ = model\n",
    "    print(\"‚úì Model and tokenizer are loaded\")\n",
    "except NameError:\n",
    "    print(\"‚ùå ERROR: Model not loaded!\")\n",
    "    print(\"\\nüîÑ STEPS TO FIX:\")\n",
    "    print(\"1. Scroll up to Step 2, Cell 11 (Load Model for Summarization)\")\n",
    "    print(\"2. Run that cell to load the Qwen model\")\n",
    "    print(\"3. Wait for 'Model loaded successfully!' message\")\n",
    "    print(\"4. Then come back here and run this cell again\")\n",
    "    raise NameError(\"Model and tokenizer must be loaded before summarization. Run Cell 11 first!\")\n",
    "\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "\n",
    "def summarize_scene(scene_content, max_tokens=200):\n",
    "    system_prompt = \"\"\"You are a professional film critic. Summarize scenes clearly and concisely.\n",
    "Focus on key events, character interactions, and plot developments.\n",
    "Write in narrative style, not bullet points.\"\"\"\n",
    "\n",
    "    user_prompt = f\"\"\"Summarize this scene:\n",
    "\n",
    "{scene_content}\n",
    "\n",
    "Provide a clear, narrative summary (1-2 paragraphs).\"\"\"\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    "\n",
    "    input_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    \n",
    "    # Tokenize with aggressive truncation\n",
    "    inputs = tokenizer(\n",
    "        input_text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=1200  # Reduce input size\n",
    "    ).to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_tokens,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            use_cache=False  # Disable KV cache to save memory\n",
    "        )\n",
    "\n",
    "    full_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    if \"assistant\\n\\n\" in full_output:\n",
    "        summary = full_output.split(\"assistant\\n\\n\")[-1].strip()\n",
    "    else:\n",
    "        summary = full_output\n",
    "    \n",
    "    # Aggressive memory cleanup\n",
    "    del inputs, outputs\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    return summary\n",
    "\n",
    "# Generate map summaries with memory monitoring\n",
    "map_summaries = []\n",
    "for i, rep in enumerate(tqdm(representatives, desc=\"MAP phase\")):\n",
    "    summary = summarize_scene(rep['content'])\n",
    "    map_summaries.append({\n",
    "        \"scene_number\": rep['scene_number'],\n",
    "        \"header\": rep['header'],\n",
    "        \"summary\": summary\n",
    "    })\n",
    "    \n",
    "    # Clear cache after every scene\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    # Print memory status every 3 scenes\n",
    "    if (i + 1) % 3 == 0:\n",
    "        mem = torch.cuda.memory_allocated() / 1e9\n",
    "        print(f\"  [{i+1}/{len(representatives)}] Memory: {mem:.2f}GB\")\n",
    "\n",
    "print(f\"\\n‚úì Generated {len(map_summaries)} scene summaries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e726fbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# REDUCE phase: Combine into final summary (with memory optimization)\n",
    "import gc\n",
    "\n",
    "def combine_summaries(summaries, max_tokens=400):\n",
    "    system_prompt = \"\"\"You are a professional film critic writing a complete screenplay summary.\n",
    "Combine the scene summaries into one coherent, chronological narrative.\n",
    "Be concise but comprehensive.\"\"\"\n",
    "\n",
    "    # More compact joining\n",
    "    combined_text = \"\\n\".join([s['summary'] for s in summaries])\n",
    "\n",
    "    user_prompt = f\"\"\"Combine these scene summaries into one complete story:\n",
    "\n",
    "{combined_text}\n",
    "\n",
    "Write a concise narrative summary.\"\"\"\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    "\n",
    "    input_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    \n",
    "    # Tokenize with aggressive truncation\n",
    "    inputs = tokenizer(\n",
    "        input_text, \n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=1500  # Stricter limit\n",
    "    ).to(model.device)\n",
    "\n",
    "    print(\"Generating final summary...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_tokens,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            use_cache=False  # Disable KV cache\n",
    "        )\n",
    "\n",
    "    full_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    if \"assistant\\n\\n\" in full_output:\n",
    "        summary = full_output.split(\"assistant\\n\\n\")[-1].strip()\n",
    "    else:\n",
    "        summary = full_output\n",
    "    \n",
    "    # Cleanup\n",
    "    del inputs, outputs\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    return summary\n",
    "\n",
    "# Clear memory before REDUCE\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "final_summary = combine_summaries(map_summaries)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(final_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e1cded",
   "metadata": {},
   "source": [
    "## 7. Save to MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711bbf3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mongodb_manager import MongoDBManager\n",
    "\n",
    "# Connect to MongoDB (use MongoDB Atlas for cloud)\n",
    "mongodb_uri = \"mongodb+srv://<username>:<password>@cluster.mongodb.net/\"\n",
    "mongo = MongoDBManager(uri=mongodb_uri, db_name=\"screenplay_summaries\")\n",
    "\n",
    "# Save summary\n",
    "doc_id = mongo.save_summary(\n",
    "    title=\"Your Show S01E01\",\n",
    "    final_summary=final_summary,\n",
    "    map_outputs=map_summaries,\n",
    "    metadata={\n",
    "        \"show\": \"Your Show\",\n",
    "        \"season\": 1,\n",
    "        \"episode\": 1,\n",
    "        \"processed_on\": \"colab\"\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"‚úì Saved to MongoDB with ID: {doc_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a488fd1",
   "metadata": {},
   "source": [
    "## 8. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4335b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final summary to file\n",
    "with open(\"summaries/final/your_show_s01e01.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(final_summary)\n",
    "\n",
    "print(\"‚úì Summary saved to file\")\n",
    "\n",
    "# Download to local machine\n",
    "from google.colab import files\n",
    "files.download(\"summaries/final/your_show_s01e01.txt\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
